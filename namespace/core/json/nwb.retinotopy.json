{"groups":[{"datasets":[{"attributes":[{"dims":["num_rows, num_cols"],"doc":"Number of rows and columns in the image. NOTE: row, column representation is equivalent to height, width.","dtype":"int32","name":"dimension","shape":[2]},{"dims":["height, width"],"doc":"Size of viewing area, in meters.","dtype":"float32","name":"field_of_view","shape":[2]},{"doc":"Unit that axis data is stored in (e.g., degrees).","dtype":"text","name":"unit"}],"dims":["num_rows","num_cols"],"doc":"Phase response to stimulus on the first measured axis.","dtype":"float32","name":"axis_1_phase_map","shape":[null,null]},{"attributes":[{"dims":["num_rows, num_cols"],"doc":"Number of rows and columns in the image. NOTE: row, column representation is equivalent to height, width.","dtype":"int32","name":"dimension","shape":[2]},{"dims":["height, width"],"doc":"Size of viewing area, in meters.","dtype":"float32","name":"field_of_view","shape":[2]},{"doc":"Unit that axis data is stored in (e.g., degrees).","dtype":"text","name":"unit"}],"dims":["num_rows","num_cols"],"doc":"Power response on the first measured axis. Response is scaled so 0.0 is no power in the response and 1.0 is maximum relative power.","dtype":"float32","name":"axis_1_power_map","quantity":"?","shape":[null,null]},{"attributes":[{"dims":["num_rows, num_cols"],"doc":"Number of rows and columns in the image. NOTE: row, column representation is equivalent to height, width.","dtype":"int32","name":"dimension","shape":[2]},{"dims":["height, width"],"doc":"Size of viewing area, in meters.","dtype":"float32","name":"field_of_view","shape":[2]},{"doc":"Unit that axis data is stored in (e.g., degrees).","dtype":"text","name":"unit"}],"dims":["num_rows","num_cols"],"doc":"Phase response to stimulus on the second measured axis.","dtype":"float32","name":"axis_2_phase_map","shape":[null,null]},{"attributes":[{"dims":["num_rows, num_cols"],"doc":"Number of rows and columns in the image. NOTE: row, column representation is equivalent to height, width.","dtype":"int32","name":"dimension","shape":[2]},{"dims":["height, width"],"doc":"Size of viewing area, in meters.","dtype":"float32","name":"field_of_view","shape":[2]},{"doc":"Unit that axis data is stored in (e.g., degrees).","dtype":"text","name":"unit"}],"dims":["num_rows","num_cols"],"doc":"Power response on the second measured axis. Response is scaled so 0.0 is no power in the response and 1.0 is maximum relative power.","dtype":"float32","name":"axis_2_power_map","quantity":"?","shape":[null,null]},{"dims":["axis_1, axis_2"],"doc":"Two-element array describing the contents of the two response axis fields. Description should be something like ['altitude', 'azimuth'] or '['radius', 'theta'].","dtype":"text","name":"axis_descriptions","shape":[2]},{"attributes":[{"doc":"Number of bits used to represent each value. This is necessary to determine maximum (white) pixel value.","dtype":"int32","name":"bits_per_pixel"},{"dims":["num_rows, num_cols"],"doc":"Number of rows and columns in the image. NOTE: row, column representation is equivalent to height, width.","dtype":"int32","name":"dimension","shape":[2]},{"dims":["height, width"],"doc":"Size of viewing area, in meters.","dtype":"float32","name":"field_of_view","shape":[2]},{"doc":"Focal depth offset, in meters.","dtype":"float32","name":"focal_depth"},{"doc":"Format of image. Right now only 'raw' is supported.","dtype":"text","name":"format"}],"dims":["num_rows","num_cols"],"doc":"Gray-scale image taken with same settings/parameters (e.g., focal depth, wavelength) as data collection. Array format: [rows][columns].","dtype":"uint16","name":"focal_depth_image","quantity":"?","shape":[null,null]},{"attributes":[{"dims":["num_rows, num_cols"],"doc":"Number of rows and columns in the image. NOTE: row, column representation is equivalent to height, width.","dtype":"int32","name":"dimension","shape":[2]},{"dims":["height, width"],"doc":"Size of viewing area, in meters.","dtype":"float32","name":"field_of_view","shape":[2]}],"dims":["num_rows","num_cols"],"doc":"Sine of the angle between the direction of the gradient in axis_1 and axis_2.","dtype":"float32","name":"sign_map","quantity":"?","shape":[null,null]},{"attributes":[{"doc":"Number of bits used to represent each value. This is necessary to determine maximum (white) pixel value","dtype":"int32","name":"bits_per_pixel"},{"dims":["num_rows, num_cols"],"doc":"Number of rows and columns in the image. NOTE: row, column representation is equivalent to height, width.","dtype":"int32","name":"dimension","shape":[2]},{"dims":["height, width"],"doc":"Size of viewing area, in meters.","dtype":"float32","name":"field_of_view","shape":[2]},{"doc":"Format of image. Right now only 'raw' is supported.","dtype":"text","name":"format"}],"dims":["num_rows","num_cols"],"doc":"Gray-scale anatomical image of cortical surface. Array structure: [rows][columns]","dtype":"uint16","name":"vasculature_image","shape":[null,null]}],"default_name":"ImagingRetinotopy","doc":"Intrinsic signal optical imaging or widefield imaging for measuring retinotopy. Stores orthogonal maps (e.g., altitude/azimuth; radius/theta) of responses to specific stimuli and a combined polarity map from which to identify visual areas. This group does not store the raw responses imaged during retinotopic mapping or the stimuli presented, but rather the resulting phase and power maps after applying a Fourier transform on the averaged responses. Note: for data consistency, all images and arrays are stored in the format [row][column] and [row, col], which equates to [y][x]. Field of view and dimension arrays may appear backward (i.e., y before x).","neurodata_type_def":"ImagingRetinotopy","neurodata_type_inc":"NWBDataInterface"}]}
